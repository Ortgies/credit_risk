{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2434be5c",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "I want to use an autoencoder for dimensionality reduction on the full dataset with 1400+ features. <br>\n",
    "At the moment, the autoencoder has the following structure. <br>\n",
    "<br>\n",
    "\n",
    "![Diagram](AE_diagram.png)\n",
    "<br>\n",
    "The input variables are separated into categorical and continuous features. The categorical features are embedded and dropout is added. The continous features are treated with a batchnorm layer. Afterwards, all features are concatenate into a single Tensor. <br>\n",
    "The encoder is still very basic, consisting of two linear and one ReLu layer, reducing the dimensionality to 128 features. When using the full dataset with over 1400 features, I will probably add another linear layer (plus ReLu) going 512->256->128. <br>\n",
    "The decoder is the exact inverse of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a89d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8b9883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d560c7cf",
   "metadata": {},
   "source": [
    "For now, I'll use the smaller, manually reduced, dataset to make testing faster. When everything is working well, I will run this on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6433b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0546217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DAYS_BIRTH'] = df['DAYS_BIRTH'].astype('float64')\n",
    "df['DAYS_ID_PUBLISH'] = df['DAYS_ID_PUBLISH'].astype('float64')\n",
    "df['SK_ID_CURR'] = df['SK_ID_CURR'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c36cc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>...</th>\n",
       "      <th>client_installments_AMT_INSTALMENT_max_mean</th>\n",
       "      <th>client_installments_AMT_PAYMENT_sum_min</th>\n",
       "      <th>client_installments_AMT_INSTALMENT_max_max</th>\n",
       "      <th>client_installments_AMT_PAYMENT_sum_mean</th>\n",
       "      <th>client_installments_AMT_PAYMENT_min_sum</th>\n",
       "      <th>client_installments_AMT_INSTALMENT_min_sum</th>\n",
       "      <th>client_installments_AMT_PAYMENT_sum_max</th>\n",
       "      <th>client_installments_AMT_PAYMENT_mean_sum</th>\n",
       "      <th>client_installments_AMT_INSTALMENT_max_sum</th>\n",
       "      <th>client_installments_AMT_PAYMENT_sum_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53093.746</td>\n",
       "      <td>219625.700</td>\n",
       "      <td>53093.746</td>\n",
       "      <td>219625.700</td>\n",
       "      <td>175783.73</td>\n",
       "      <td>175783.73</td>\n",
       "      <td>219625.700</td>\n",
       "      <td>219625.690</td>\n",
       "      <td>1008781.200</td>\n",
       "      <td>4.172888e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-16765.0</td>\n",
       "      <td>...</td>\n",
       "      <td>175764.050</td>\n",
       "      <td>80773.380</td>\n",
       "      <td>560835.400</td>\n",
       "      <td>453952.220</td>\n",
       "      <td>1154108.20</td>\n",
       "      <td>1154108.20</td>\n",
       "      <td>1150977.400</td>\n",
       "      <td>1618864.600</td>\n",
       "      <td>4394101.500</td>\n",
       "      <td>1.134881e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10573.965</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>10573.965</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>16071.75</td>\n",
       "      <td>16071.75</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>31721.895</td>\n",
       "      <td>6.386539e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>-19005.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66116.266</td>\n",
       "      <td>25091.324</td>\n",
       "      <td>691786.900</td>\n",
       "      <td>232499.700</td>\n",
       "      <td>994476.70</td>\n",
       "      <td>994476.70</td>\n",
       "      <td>691786.900</td>\n",
       "      <td>1007153.400</td>\n",
       "      <td>1057860.200</td>\n",
       "      <td>3.719995e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-19932.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12677.324</td>\n",
       "      <td>18330.390</td>\n",
       "      <td>22678.785</td>\n",
       "      <td>172669.890</td>\n",
       "      <td>483756.38</td>\n",
       "      <td>825845.80</td>\n",
       "      <td>280199.700</td>\n",
       "      <td>806127.940</td>\n",
       "      <td>836703.400</td>\n",
       "      <td>1.139621e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 293 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0    100002.0     1.0         Cash loans           M    406597.5      24700.5   \n",
       "1    100003.0     0.0         Cash loans           F   1293502.5      35698.5   \n",
       "2    100004.0     0.0    Revolving loans           M    135000.0       6750.0   \n",
       "3    100006.0     0.0         Cash loans           F    312682.5      29686.5   \n",
       "4    100007.0     0.0         Cash loans           M    513000.0      21865.5   \n",
       "\n",
       "             NAME_EDUCATION_TYPE    NAME_FAMILY_STATUS  \\\n",
       "0  Secondary / secondary special  Single / not married   \n",
       "1               Higher education               Married   \n",
       "2  Secondary / secondary special  Single / not married   \n",
       "3  Secondary / secondary special        Civil marriage   \n",
       "4  Secondary / secondary special  Single / not married   \n",
       "\n",
       "   REGION_POPULATION_RELATIVE  DAYS_BIRTH  ...  \\\n",
       "0                    0.018801     -9461.0  ...   \n",
       "1                    0.003541    -16765.0  ...   \n",
       "2                    0.010032    -19046.0  ...   \n",
       "3                    0.008019    -19005.0  ...   \n",
       "4                    0.028663    -19932.0  ...   \n",
       "\n",
       "   client_installments_AMT_INSTALMENT_max_mean  \\\n",
       "0                                    53093.746   \n",
       "1                                   175764.050   \n",
       "2                                    10573.965   \n",
       "3                                    66116.266   \n",
       "4                                    12677.324   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_sum_min  \\\n",
       "0                               219625.700   \n",
       "1                                80773.380   \n",
       "2                                21288.465   \n",
       "3                                25091.324   \n",
       "4                                18330.390   \n",
       "\n",
       "   client_installments_AMT_INSTALMENT_max_max  \\\n",
       "0                                   53093.746   \n",
       "1                                  560835.400   \n",
       "2                                   10573.965   \n",
       "3                                  691786.900   \n",
       "4                                   22678.785   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_sum_mean  \\\n",
       "0                                219625.700   \n",
       "1                                453952.220   \n",
       "2                                 21288.465   \n",
       "3                                232499.700   \n",
       "4                                172669.890   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_min_sum  \\\n",
       "0                                175783.73   \n",
       "1                               1154108.20   \n",
       "2                                 16071.75   \n",
       "3                                994476.70   \n",
       "4                                483756.38   \n",
       "\n",
       "  client_installments_AMT_INSTALMENT_min_sum  \\\n",
       "0                                  175783.73   \n",
       "1                                 1154108.20   \n",
       "2                                   16071.75   \n",
       "3                                  994476.70   \n",
       "4                                  825845.80   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_sum_max  \\\n",
       "0                               219625.700   \n",
       "1                              1150977.400   \n",
       "2                                21288.465   \n",
       "3                               691786.900   \n",
       "4                               280199.700   \n",
       "\n",
       "  client_installments_AMT_PAYMENT_mean_sum  \\\n",
       "0                               219625.690   \n",
       "1                              1618864.600   \n",
       "2                                21288.465   \n",
       "3                              1007153.400   \n",
       "4                               806127.940   \n",
       "\n",
       "   client_installments_AMT_INSTALMENT_max_sum  \\\n",
       "0                                 1008781.200   \n",
       "1                                 4394101.500   \n",
       "2                                   31721.895   \n",
       "3                                 1057860.200   \n",
       "4                                  836703.400   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_sum_sum  \n",
       "0                             4.172888e+06  \n",
       "1                             1.134881e+07  \n",
       "2                             6.386539e+04  \n",
       "3                             3.719995e+06  \n",
       "4                             1.139621e+07  \n",
       "\n",
       "[5 rows x 293 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "631d7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].dtype != 'float64':\n",
    "        df[column] = df[column].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e4314",
   "metadata": {},
   "source": [
    "To determine the size of categorical feature embeddings, I use a rule of thumb developed by the FastAI team. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbb20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_sz_rule(n_cat): return round(1.6 * n_cat**0.56) ## rule of thumb\n",
    "\n",
    "def def_emb_sz(df):\n",
    "    sz_dict = {}\n",
    "    n_cat = df.nunique().values\n",
    "    sz = [(x, emb_sz_rule(x)) for x in n_cat]\n",
    "\n",
    "    return sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de011d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a helper function that takes in the base dataset, does some preprocessing and returns the dataset split into train/test and categorical/continuous variables\n",
    "def load_data(df, valid_size=0.2):\n",
    "    x = df.drop(columns=['SK_ID_CURR', 'TARGET', 'test'])\n",
    "    x = x.replace([np.inf, -np.inf], np.nan) ## only needed for the reduced dataset, remove this when ready\n",
    "\n",
    "    ## train test split\n",
    "    ix = int(len(df)*valid_size)\n",
    "    x = x.sample(frac=1).reset_index() ## shuffle\n",
    "    x['valid'] = False\n",
    "    x.loc[0:ix, 'valid'] = True\n",
    "\n",
    "    \n",
    "    ## handle categorical variables\n",
    "    x_cat = x.select_dtypes(['category', 'bool'])\n",
    "    x_cat = x_cat.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "    emb_sz = def_emb_sz(x_cat)[:-1] ## the last element is the \"valid\" variable, drop it\n",
    "    x_cat_train, x_cat_test = x_cat[x_cat['valid'] == False].drop(columns='valid'), x_cat[x_cat['valid'] == True].drop(columns='valid')\n",
    "    x_cat_train, x_cat_test = x_cat_train.values.reshape(-1, x_cat_train.shape[1]).astype('int64'), x_cat_test.values.reshape(-1, x_cat_test.shape[1]).astype('int64')\n",
    "    \n",
    "    ## handle continuous variables\n",
    "    x_cont = x.select_dtypes(['float64', 'bool'])\n",
    "    x_cont = x_cont.fillna(0)\n",
    "    x_cont_train, x_cont_test = x_cont[x_cont['valid'] == False].drop(columns='valid'), x_cont[x_cont['valid'] == True].drop(columns='valid')\n",
    "    x_cont_train, x_cont_test = x_cont_train.values.reshape(-1, x_cont_train.shape[1]).astype('float32'), x_cont_test.values.reshape(-1, x_cont_test.shape[1]).astype('float32')\n",
    "    \n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x_cont_train = standardizer.fit_transform(x_cont_train) ## only fit on train set\n",
    "    x_cont_test = standardizer.transform(x_cont_test)\n",
    "    \n",
    "\n",
    "    return x_cat_train, x_cat_test, x_cont_train, x_cont_test, emb_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b85c726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom dataset class\n",
    "class AEDataset(Dataset):\n",
    "    def __init__(self, x_cat, x_cont, emb_sz):\n",
    "        self.x_cat, self.x_cont,self.emb_sz = x_cat, x_cont, emb_sz\n",
    "        self.n_cont = self.x_cont.shape[1]\n",
    "        self.n_cat = self.x_cat.shape[1]\n",
    "        self.len = self.n_cont + self.n_cat\n",
    "\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x_cont[index], self.x_cat[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4587760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df):\n",
    "    x_cat_train, x_cat_test, x_cont_train, x_cont_test, emb_sz = load_data(df)\n",
    "    \n",
    "    train_dataset = AEDataset(x_cat_train, x_cont_train, emb_sz)\n",
    "    test_dataset = AEDataset(x_cat_test, x_cont_test, emb_sz)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ac0301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = get_datasets(df)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,batch_size=1024)\n",
    "test_dataloader = DataLoader(dataset=test_dataset,batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e2cafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0565beb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0346,  0.2323,  0.3076,  ..., -0.6898, -0.4504, -0.4212],\n",
       "        [ 0.7837, -0.0751, -0.7841,  ...,  0.5513, -0.0537, -0.2433],\n",
       "        [-0.5713, -0.0833,  1.8241,  ..., -0.4208, -0.3381, -0.3476],\n",
       "        ...,\n",
       "        [ 0.2191,  0.4788,  0.7103,  ...,  0.5625,  0.5897,  0.0177],\n",
       "        [ 2.4047,  1.0345,  1.8241,  ..., -0.3679, -0.3985, -0.3512],\n",
       "        [-0.9100, -1.1109,  0.3961,  ..., -0.6474, -0.4444, -0.3829]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89f914cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False,  ..., False,  True,  True],\n",
       "        [False,  True,  True,  ..., False,  True, False],\n",
       "        [False,  True,  True,  ..., False,  True, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ..., False, False,  True],\n",
       "        [False,  True, False,  ..., False, False, False],\n",
       "        [False,  True,  True,  ..., False,  True, False]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.rand(x.size()) > (1 - 0.5)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "547d1150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[202, 152, 113,  ...,   4, 175, 223],\n",
       "        [ 26, 268,  88,  ...,  19, 196,  81],\n",
       "        [152, 121,  11,  ..., 247, 153, 240],\n",
       "        ...,\n",
       "        [ 76, 177,  78,  ..., 132,  31, 215],\n",
       "        [ 82, 124, 266,  ..., 137, 208, 160],\n",
       "        [141, 255, 216,  ...,  51,  71, 126]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = torch.floor(torch.rand(x.size()) * x.size(0)).type(torch.LongTensor)\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99a82491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[276, 276,   0,  ...,   0, 276, 276],\n",
       "        [  0, 276, 276,  ...,   0, 276,   0],\n",
       "        [  0, 276, 276,  ...,   0, 276,   0],\n",
       "        ...,\n",
       "        [276, 276, 276,  ...,   0,   0, 276],\n",
       "        [  0, 276,   0,  ...,   0,   0,   0],\n",
       "        [  0, 276, 276,  ...,   0, 276,   0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = (mask.type(torch.LongTensor) * x.size(1))\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca8043ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([55752, 41952,     0,  ...,     0, 19596,     0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = (l1 * l2).view(-1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2dd6a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([55752, 41953,     2,  ..., 80037, 99634, 80039])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.arange(x.nelement()) + res\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e800470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([55752, 41953,     2,  ..., 80037, 19594, 80039])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[idx>=x.nelement()] = idx[idx>=x.nelement()]-x.nelement()\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c328b287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4047,  0.2537,  0.3076,  ..., -0.6898, -0.1157, -0.1346],\n",
       "        [ 0.7837,  0.5534,  1.0731,  ...,  0.5513, -0.4033, -0.2433],\n",
       "        [-0.5713, -0.8320, -0.2062,  ..., -0.4208, -0.4448, -0.3476],\n",
       "        ...,\n",
       "        [-0.4215, -0.9610, -0.1290,  ...,  0.5625,  0.5897, -0.3633],\n",
       "        [ 2.4047, -0.3035,  1.8241,  ..., -0.3679, -0.3985, -0.3512],\n",
       "        [-0.9100, -0.4953, -0.1506,  ..., -0.6474, -0.4331, -0.3829]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flatten()[idx].view(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3dbe5",
   "metadata": {},
   "source": [
    "A very efficient way of adding noise to an input batch is Swap Noise. With a given probability p, a feature value is replaced with a random value of the same feature distribution. <br>\n",
    "The actual implementation is copied from [EtienneT's](https://github.com/EtienneT/TabularVAE/blob/master/TabularAE.ipynb) Variational Autoencoder for FastAI and adapted for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa516dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSwapNoise(torch.nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mask = torch.rand(x.size()) > (1 - self.p)\n",
    "            l1 = torch.floor(torch.rand(x.size()) * x.size(0)).type(torch.LongTensor)\n",
    "            l2 = (mask.type(torch.LongTensor) * x.size(1))\n",
    "            res = (l1 * l2).view(-1)\n",
    "            idx = torch.arange(x.nelement()) + res\n",
    "            idx[idx>=x.nelement()] = idx[idx>=x.nelement()]-x.nelement()\n",
    "            return x.flatten()[idx].view(x.size())\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "061d408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in emb_szs])  \n",
    "        n_emb = sum([size for categories,size in emb_szs]) #sum embedding sizes\n",
    "        n_cat = sum([categories for categories,size in emb_szs]) ## sum category labels\n",
    "        self.n_emb, self.n_cont, self.n_cat = n_emb, n_cont, n_cat\n",
    "        self.n_input = n_emb + n_cont\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "        self.bn_cont = nn.BatchNorm1d(self.n_cont)\n",
    "        self.noise = BatchSwapNoise(0.1)\n",
    "\n",
    "         \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_input, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "        )\n",
    "          \n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, self.n_input),\n",
    "        )\n",
    "        \n",
    "    \n",
    "    ## function to turn embeddings into label probabilities\n",
    "    def decode_embeddings(self, values, proba=False):\n",
    "        softmax = torch.nn.Softmax(dim=0)\n",
    "        \n",
    "        start = 0\n",
    "\n",
    "        out = torch.empty((290,0), requires_grad=True).long().to(device)\n",
    "\n",
    "        ### loop through each embedding\n",
    "        ## TODO: there has to be a better way to do this than two loops\n",
    "        for emb in self.embeddings:\n",
    "            \n",
    "            stop = start + emb.weight.data.shape[1]\n",
    "            \n",
    "            pred = torch.empty((0,emb.weight.data.shape[0])).to(device) ## create empty tensor of size (0,n) where n is the number of feature labels\n",
    "                \n",
    "            ## loop through each item in the batch   \n",
    "            for i in range(290):\n",
    "                \n",
    "                ## get the euclidian distance between input values and feature embeddings\n",
    "                distance = ((emb.weight.data - values[i,start:stop])**2).sum(axis=1)\n",
    " \n",
    "                ## softmax assigns the highest probability to the highest input value\n",
    "                ## I take the negative to give assign the highest probability to the smallest distance\n",
    "                p = softmax(-distance).unsqueeze(0)\n",
    "                    \n",
    "                pred = torch.cat([pred,p])\n",
    "                    \n",
    "            out = torch.cat([out,pred],axis=1)\n",
    "            start = stop  \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, x_cont, x_cat, encode=False):\n",
    "        x_cat = x_cat.to(device)\n",
    "        x_cont = x_cont.to(device)\n",
    "        \n",
    "        ## add batch swap noise\n",
    "        x_cat = self.noise(x_cat)\n",
    "        x_cont = self.noise(x_cont)\n",
    "\n",
    "        ## embedd categorical features and add dropout\n",
    "        x = [emb(x_cat[:,i]) for i, emb in enumerate(self.embeddings)]   \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        ## batch norm on continuous features\n",
    "        x2 = self.bn_cont(x_cont)\n",
    "        \n",
    "        ## concat cat and cont features\n",
    "        x = torch.cat([x, x2], 1) \n",
    "        \n",
    "        ## encode features\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        ## if I only want to encode features, stop here\n",
    "        if encode:\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        ## decode encoded features\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        ## get continuous features\n",
    "        x_cont_out = x[:,self.n_emb:]\n",
    "        \n",
    "        ## get categorical features and reverse embedding\n",
    "        x_cat_out = x[:,0:self.n_emb]\n",
    "        x_cat_out = self.decode_embeddings(x_cat_out)\n",
    "        \n",
    "        return x_cont_out, x_cat_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b166ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_x = train_dataset.len\n",
    "len_cont = train_dataset.n_cont\n",
    "emb_sz = train_dataset.emb_sz\n",
    "model = AutoEncoder(emb_sz, len_cont).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38f0b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    def forward(self, x_cont_ae, x_cat_ae, x_cont, x_cat):\n",
    "\n",
    "        ## loss for the continuous features is simply MSE\n",
    "        cont_loss = self.mse_loss(x_cont_ae, x_cont)\n",
    "        \n",
    "        ## the input categorical features are label encoded, but one-hot encoding is needed to calculate cross entropy loss\n",
    "        one_hot = torch.empty((290,0), requires_grad=True).long().to(device)\n",
    "        \n",
    "        for i in range(x_cat.shape[1]):\n",
    "            n_classes = train_dataset.x_cat[:,i].max() + 1 ## not every label is present in a batch, so I have to supply the amount of feature labels   \n",
    "            out = nn.functional.one_hot(x_cat[:,i], num_classes=n_classes).to(device)\n",
    "            one_hot = torch.cat([one_hot,out], axis=1)\n",
    "        \n",
    "        one_hot = one_hot.float()\n",
    "     \n",
    "        ## now I can easily calculate cross entropy loss\n",
    "        cat_loss = self.ce_loss(x_cat_ae, one_hot)\n",
    "        \n",
    "        ## take the sum as the final output\n",
    "        loss = cont_loss + cat_loss\n",
    "        \n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed8b2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = customLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = 1e-2,\n",
    "                             weight_decay = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35242603",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c5be1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 301.24. Valid loss: 331.35.\n",
      "Epoch 1. Train loss: 291.57. Valid loss: 320.52.\n",
      "Epoch 2. Train loss: 275.46. Valid loss: 303.82.\n",
      "Epoch 3. Train loss: 267.91. Valid loss: 286.43.\n",
      "Epoch 4. Train loss: 243.76. Valid loss: 277.16.\n",
      "Epoch 5. Train loss: 233.16. Valid loss: 263.99.\n",
      "Epoch 6. Train loss: 216.29. Valid loss: 265.88.\n",
      "Epoch 7. Train loss: 213.83. Valid loss: 253.67.\n",
      "Epoch 8. Train loss: 200.39. Valid loss: 249.48.\n",
      "Epoch 9. Train loss: 195.94. Valid loss: 244.03.\n",
      "Epoch 10. Train loss: 187.75. Valid loss: 242.87.\n",
      "Epoch 11. Train loss: 183.24. Valid loss: 237.38.\n",
      "Epoch 12. Train loss: 177.26. Valid loss: 234.62.\n",
      "Epoch 13. Train loss: 172.15. Valid loss: 232.02.\n",
      "Epoch 14. Train loss: 167.61. Valid loss: 228.26.\n",
      "Epoch 15. Train loss: 162.89. Valid loss: 225.25.\n",
      "Epoch 16. Train loss: 158.37. Valid loss: 224.75.\n",
      "Epoch 17. Train loss: 154.53. Valid loss: 223.28.\n",
      "Epoch 18. Train loss: 150.21. Valid loss: 221.30.\n",
      "Epoch 19. Train loss: 146.99. Valid loss: 219.59.\n",
      "Epoch 20. Train loss: 143.57. Valid loss: 218.99.\n",
      "Epoch 21. Train loss: 140.44. Valid loss: 217.49.\n",
      "Epoch 22. Train loss: 137.50. Valid loss: 216.06.\n",
      "Epoch 23. Train loss: 134.82. Valid loss: 216.07.\n",
      "Epoch 24. Train loss: 132.37. Valid loss: 215.43.\n",
      "Epoch 25. Train loss: 129.35. Valid loss: 214.48.\n",
      "Epoch 26. Train loss: 126.71. Valid loss: 213.34.\n",
      "Epoch 27. Train loss: 124.63. Valid loss: 213.15.\n",
      "Epoch 28. Train loss: 122.58. Valid loss: 212.27.\n",
      "Epoch 29. Train loss: 121.05. Valid loss: 211.26.\n",
      "Epoch 30. Train loss: 118.61. Valid loss: 211.61.\n",
      "Epoch 31. Train loss: 116.43. Valid loss: 211.12.\n",
      "Epoch 32. Train loss: 115.35. Valid loss: 210.64.\n",
      "Epoch 33. Train loss: 113.52. Valid loss: 210.71.\n",
      "Epoch 34. Train loss: 112.14. Valid loss: 210.34.\n",
      "Epoch 35. Train loss: 111.00. Valid loss: 209.47.\n",
      "Epoch 36. Train loss: 108.41. Valid loss: 209.54.\n",
      "Epoch 37. Train loss: 107.97. Valid loss: 209.74.\n",
      "Epoch 38. Train loss: 106.95. Valid loss: 209.96.\n",
      "Epoch 39. Train loss: 104.68. Valid loss: 211.08.\n",
      "Epoch 40. Train loss: 104.60. Valid loss: 210.09.\n",
      "Epoch 41. Train loss: 103.04. Valid loss: 209.86.\n",
      "Epoch 42. Train loss: 101.99. Valid loss: 209.90.\n",
      "Epoch 43. Train loss: 100.28. Valid loss: 209.93.\n",
      "Epoch 44. Train loss: 99.79. Valid loss: 210.63.\n",
      "Epoch 45. Train loss: 98.91. Valid loss: 209.44.\n",
      "Epoch 46. Train loss: 98.20. Valid loss: 209.40.\n",
      "Epoch 47. Train loss: 96.31. Valid loss: 210.63.\n",
      "Epoch 48. Train loss: 95.63. Valid loss: 210.18.\n",
      "Epoch 49. Train loss: 95.35. Valid loss: 210.04.\n",
      "Epoch 50. Train loss: 94.53. Valid loss: 210.00.\n",
      "Epoch 51. Train loss: 94.12. Valid loss: 210.43.\n",
      "Epoch 52. Train loss: 93.35. Valid loss: 210.20.\n",
      "Epoch 53. Train loss: 92.37. Valid loss: 209.44.\n",
      "Epoch 54. Train loss: 91.07. Valid loss: 208.89.\n",
      "Epoch 55. Train loss: 90.19. Valid loss: 209.50.\n",
      "Epoch 56. Train loss: 90.01. Valid loss: 208.91.\n",
      "Epoch 57. Train loss: 89.47. Valid loss: 208.40.\n",
      "Epoch 58. Train loss: 89.00. Valid loss: 208.86.\n",
      "Epoch 59. Train loss: 88.53. Valid loss: 209.02.\n",
      "Epoch 60. Train loss: 87.46. Valid loss: 209.25.\n",
      "Epoch 61. Train loss: 87.90. Valid loss: 208.99.\n",
      "Epoch 62. Train loss: 87.02. Valid loss: 209.14.\n",
      "Epoch 63. Train loss: 86.27. Valid loss: 209.38.\n",
      "Epoch 64. Train loss: 85.85. Valid loss: 208.96.\n",
      "Epoch 65. Train loss: 86.31. Valid loss: 209.46.\n",
      "Epoch 66. Train loss: 84.92. Valid loss: 209.38.\n",
      "Epoch 67. Train loss: 85.42. Valid loss: 208.80.\n",
      "No improvement for 10 epochs. Ending training.\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    \n",
    "    ## train\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        x_cont, x_cat = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_cont_ae, x_cat_ae = model(x_cont, x_cat)\n",
    "        \n",
    "        loss = loss_function(x_cont_ae, x_cat_ae, x_cont, x_cat)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "     \n",
    "    \n",
    "    ## validate\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch_idx, data in enumerate(test_dataloader):\n",
    "        x_cont, x_cat = data[0].to(device), data[1].to(device)\n",
    "         \n",
    "        x_cont_ae, x_cat_ae = model(x_cont, x_cat)\n",
    "        \n",
    "        loss = loss_function(x_cont_ae, x_cat_ae, x_cont, x_cat)\n",
    "        \n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    train_losses.append(train_loss / len(train_dataloader.dataset))\n",
    "    val_losses.append(valid_loss / len(test_dataloader.dataset))\n",
    " \n",
    "    print('Epoch {}. Train loss: {:.2f}. Valid loss: {:.2f}.'.format(e, train_losses[-1], val_losses[-1]))\n",
    "    \n",
    "    if (e > patience) & (min(val_losses[-patience:]) != min(val_losses)):\n",
    "        print('No improvement for {} epochs. Ending training.'.format(patience))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a181f",
   "metadata": {},
   "source": [
    "## Preliminary Result\n",
    "The training loss is really good, the model is able to recreate the input variables very well. However, the validation loss is converging very fast. I could probably improve this by adding more regularization. <br>\n",
    "I'm not quite sure if I should even consider validation loss when my only objective is to reduce the dimensionality of a known dataset. I'll have to do some research on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "826cd898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df = pd.DataFrame({'train': train_losses, 'valid': val_losses})\n",
    "ax = sns.lineplot(data=plot_df)\n",
    "ax.set(xlabel='Epoch', ylabel='Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
