{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2434be5c",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "I want to use an autoencoder for dimensionality reduction on the full dataset with 1400+ features. <br>\n",
    "At the moment, the autoencoder has the following structure. <br>\n",
    "<br>\n",
    "\n",
    "![Diagram](AE_diagram.png)\n",
    "<br>\n",
    "The input variables are separated into categorical and continuous features. The categorical features are embedded and dropout is added. The continous features are treated with a batchnorm layer. Afterwards, all features are concatenate into a single Tensor. <br>\n",
    "The encoder is still very basic, consisting of two linear and one ReLu layer, reducing the dimensionality to 128 features. When using the full dataset with over 1400 features, I will probably add another linear layer (plus ReLu) going 512->256->128. <br>\n",
    "The decoder is the exact inverse of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a89d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8b9883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d560c7cf",
   "metadata": {},
   "source": [
    "For now, I'll use the smaller, manually reduced, dataset to make testing faster. When everything is working well, I will run this on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6433b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0546217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DAYS_BIRTH'] = df['DAYS_BIRTH'].astype('float64')\n",
    "df['DAYS_ID_PUBLISH'] = df['DAYS_ID_PUBLISH'].astype('float64')\n",
    "df['SK_ID_CURR'] = df['SK_ID_CURR'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c36cc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>...</th>\n",
       "      <th>client_installments_AMT_INSTALMENT_max_mean</th>\n",
       "      <th>client_installments_AMT_PAYMENT_sum_min</th>\n",
       "      <th>client_installments_AMT_INSTALMENT_max_max</th>\n",
       "      <th>client_installments_AMT_PAYMENT_sum_mean</th>\n",
       "      <th>client_installments_AMT_PAYMENT_min_sum</th>\n",
       "      <th>client_installments_AMT_INSTALMENT_min_sum</th>\n",
       "      <th>client_installments_AMT_PAYMENT_sum_max</th>\n",
       "      <th>client_installments_AMT_PAYMENT_mean_sum</th>\n",
       "      <th>client_installments_AMT_INSTALMENT_max_sum</th>\n",
       "      <th>client_installments_AMT_PAYMENT_sum_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53093.746</td>\n",
       "      <td>219625.700</td>\n",
       "      <td>53093.746</td>\n",
       "      <td>219625.700</td>\n",
       "      <td>175783.73</td>\n",
       "      <td>175783.73</td>\n",
       "      <td>219625.700</td>\n",
       "      <td>219625.690</td>\n",
       "      <td>1008781.200</td>\n",
       "      <td>4.172888e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-16765.0</td>\n",
       "      <td>...</td>\n",
       "      <td>175764.050</td>\n",
       "      <td>80773.380</td>\n",
       "      <td>560835.400</td>\n",
       "      <td>453952.220</td>\n",
       "      <td>1154108.20</td>\n",
       "      <td>1154108.20</td>\n",
       "      <td>1150977.400</td>\n",
       "      <td>1618864.600</td>\n",
       "      <td>4394101.500</td>\n",
       "      <td>1.134881e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10573.965</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>10573.965</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>16071.75</td>\n",
       "      <td>16071.75</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>31721.895</td>\n",
       "      <td>6.386539e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>-19005.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66116.266</td>\n",
       "      <td>25091.324</td>\n",
       "      <td>691786.900</td>\n",
       "      <td>232499.700</td>\n",
       "      <td>994476.70</td>\n",
       "      <td>994476.70</td>\n",
       "      <td>691786.900</td>\n",
       "      <td>1007153.400</td>\n",
       "      <td>1057860.200</td>\n",
       "      <td>3.719995e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-19932.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12677.324</td>\n",
       "      <td>18330.390</td>\n",
       "      <td>22678.785</td>\n",
       "      <td>172669.890</td>\n",
       "      <td>483756.38</td>\n",
       "      <td>825845.80</td>\n",
       "      <td>280199.700</td>\n",
       "      <td>806127.940</td>\n",
       "      <td>836703.400</td>\n",
       "      <td>1.139621e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 293 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0    100002.0     1.0         Cash loans           M    406597.5      24700.5   \n",
       "1    100003.0     0.0         Cash loans           F   1293502.5      35698.5   \n",
       "2    100004.0     0.0    Revolving loans           M    135000.0       6750.0   \n",
       "3    100006.0     0.0         Cash loans           F    312682.5      29686.5   \n",
       "4    100007.0     0.0         Cash loans           M    513000.0      21865.5   \n",
       "\n",
       "             NAME_EDUCATION_TYPE    NAME_FAMILY_STATUS  \\\n",
       "0  Secondary / secondary special  Single / not married   \n",
       "1               Higher education               Married   \n",
       "2  Secondary / secondary special  Single / not married   \n",
       "3  Secondary / secondary special        Civil marriage   \n",
       "4  Secondary / secondary special  Single / not married   \n",
       "\n",
       "   REGION_POPULATION_RELATIVE  DAYS_BIRTH  ...  \\\n",
       "0                    0.018801     -9461.0  ...   \n",
       "1                    0.003541    -16765.0  ...   \n",
       "2                    0.010032    -19046.0  ...   \n",
       "3                    0.008019    -19005.0  ...   \n",
       "4                    0.028663    -19932.0  ...   \n",
       "\n",
       "   client_installments_AMT_INSTALMENT_max_mean  \\\n",
       "0                                    53093.746   \n",
       "1                                   175764.050   \n",
       "2                                    10573.965   \n",
       "3                                    66116.266   \n",
       "4                                    12677.324   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_sum_min  \\\n",
       "0                               219625.700   \n",
       "1                                80773.380   \n",
       "2                                21288.465   \n",
       "3                                25091.324   \n",
       "4                                18330.390   \n",
       "\n",
       "   client_installments_AMT_INSTALMENT_max_max  \\\n",
       "0                                   53093.746   \n",
       "1                                  560835.400   \n",
       "2                                   10573.965   \n",
       "3                                  691786.900   \n",
       "4                                   22678.785   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_sum_mean  \\\n",
       "0                                219625.700   \n",
       "1                                453952.220   \n",
       "2                                 21288.465   \n",
       "3                                232499.700   \n",
       "4                                172669.890   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_min_sum  \\\n",
       "0                                175783.73   \n",
       "1                               1154108.20   \n",
       "2                                 16071.75   \n",
       "3                                994476.70   \n",
       "4                                483756.38   \n",
       "\n",
       "  client_installments_AMT_INSTALMENT_min_sum  \\\n",
       "0                                  175783.73   \n",
       "1                                 1154108.20   \n",
       "2                                   16071.75   \n",
       "3                                  994476.70   \n",
       "4                                  825845.80   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_sum_max  \\\n",
       "0                               219625.700   \n",
       "1                              1150977.400   \n",
       "2                                21288.465   \n",
       "3                               691786.900   \n",
       "4                               280199.700   \n",
       "\n",
       "  client_installments_AMT_PAYMENT_mean_sum  \\\n",
       "0                               219625.690   \n",
       "1                              1618864.600   \n",
       "2                                21288.465   \n",
       "3                              1007153.400   \n",
       "4                               806127.940   \n",
       "\n",
       "   client_installments_AMT_INSTALMENT_max_sum  \\\n",
       "0                                 1008781.200   \n",
       "1                                 4394101.500   \n",
       "2                                   31721.895   \n",
       "3                                 1057860.200   \n",
       "4                                  836703.400   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_sum_sum  \n",
       "0                             4.172888e+06  \n",
       "1                             1.134881e+07  \n",
       "2                             6.386539e+04  \n",
       "3                             3.719995e+06  \n",
       "4                             1.139621e+07  \n",
       "\n",
       "[5 rows x 293 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "631d7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].dtype != 'float64':\n",
    "        df[column] = df[column].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e4314",
   "metadata": {},
   "source": [
    "To determine the size of categorical feature embeddings, I use a rule of thumb developed by the FastAI team. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbb20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_sz_rule(n_cat): return round(1.6 * n_cat**0.56) ## rule of thumb\n",
    "\n",
    "def def_emb_sz(df):\n",
    "    sz_dict = {}\n",
    "    n_cat = df.nunique().values\n",
    "    sz = [(x, emb_sz_rule(x)) for x in n_cat]\n",
    "\n",
    "    return sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de011d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a helper function that takes in the base dataset, does some preprocessing and returns the dataset split into train/test and categorical/continuous variables\n",
    "def load_data(df, valid_size=0.2):\n",
    "    x = df.drop(columns=['SK_ID_CURR', 'TARGET', 'test'])\n",
    "    x = x.replace([np.inf, -np.inf], np.nan) ## only needed for the reduced dataset, remove this when ready\n",
    "\n",
    "    ## train test split\n",
    "    ix = int(len(df)*valid_size)\n",
    "    x = x.sample(frac=1).reset_index() ## shuffle\n",
    "    x['valid'] = False\n",
    "    x.loc[0:ix, 'valid'] = True\n",
    "\n",
    "    \n",
    "    ## handle categorical variables\n",
    "    x_cat = x.select_dtypes(['category', 'bool'])\n",
    "    x_cat = x_cat.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "    emb_sz = def_emb_sz(x_cat)[:-1] ## the last element is the \"valid\" variable, drop it\n",
    "    x_cat_train, x_cat_test = x_cat[x_cat['valid'] == False].drop(columns='valid'), x_cat[x_cat['valid'] == True].drop(columns='valid')\n",
    "    x_cat_train, x_cat_test = x_cat_train.values.reshape(-1, x_cat_train.shape[1]).astype('int64'), x_cat_test.values.reshape(-1, x_cat_test.shape[1]).astype('int64')\n",
    "    \n",
    "    ## handle continuous variables\n",
    "    x_cont = x.select_dtypes(['float64', 'bool'])\n",
    "    x_cont = x_cont.fillna(0)\n",
    "    x_cont_train, x_cont_test = x_cont[x_cont['valid'] == False].drop(columns='valid'), x_cont[x_cont['valid'] == True].drop(columns='valid')\n",
    "    x_cont_train, x_cont_test = x_cont_train.values.reshape(-1, x_cont_train.shape[1]).astype('float32'), x_cont_test.values.reshape(-1, x_cont_test.shape[1]).astype('float32')\n",
    "    \n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x_cont_train = standardizer.fit_transform(x_cont_train) ## only fit on train set\n",
    "    x_cont_test = standardizer.transform(x_cont_test)\n",
    "    \n",
    "\n",
    "    return x_cat_train, x_cat_test, x_cont_train, x_cont_test, emb_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b85c726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom dataset class\n",
    "class AEDataset(Dataset):\n",
    "    def __init__(self, x_cat, x_cont, emb_sz):\n",
    "        self.x_cat, self.x_cont,self.emb_sz = x_cat, x_cont, emb_sz\n",
    "        self.n_cont = self.x_cont.shape[1]\n",
    "        self.n_cat = self.x_cat.shape[1]\n",
    "        self.len = self.x_cat.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x_cont[index], self.x_cat[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4587760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df):\n",
    "    x_cat_train, x_cat_test, x_cont_train, x_cont_test, emb_sz = load_data(df)\n",
    "    \n",
    "    train_dataset = AEDataset(x_cat_train, x_cont_train, emb_sz)\n",
    "    test_dataset = AEDataset(x_cat_test, x_cont_test, emb_sz)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ac0301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = get_datasets(df)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,batch_size=1024)\n",
    "test_dataloader = DataLoader(dataset=test_dataset,batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14439f5a",
   "metadata": {},
   "source": [
    "A very efficient way of adding noise to an input batch is Swap Noise. With a given probability p, a feature value is replaced with a random value of the same feature distribution. <br>\n",
    "The actual implementation is copied from [EtienneT's](https://github.com/EtienneT/TabularVAE/blob/master/TabularAE.ipynb) Variational Autoencoder for FastAI and adapted for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07590f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSwapNoise(torch.nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mask = torch.rand(x.size()) > (1 - self.p)\n",
    "            l1 = torch.floor(torch.rand(x.size()) * x.size(0)).type(torch.LongTensor)\n",
    "            l2 = (mask.type(torch.LongTensor) * x.size(1))\n",
    "            res = (l1 * l2).view(-1)\n",
    "            idx = torch.arange(x.nelement()) + res\n",
    "            idx[idx>=x.nelement()] = idx[idx>=x.nelement()]-x.nelement()\n",
    "            return x.flatten()[idx].view(x.size())\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "061d408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, dropout=0.2, noise=0.1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in emb_szs])  \n",
    "        n_emb = sum([size for categories,size in emb_szs]) #sum embedding sizes\n",
    "        n_cat = sum([categories for categories,size in emb_szs]) ## sum category labels\n",
    "        self.n_emb, self.n_cont, self.n_cat = n_emb, n_cont, n_cat\n",
    "        self.n_input = n_emb + n_cont\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "        self.bn_cont = nn.BatchNorm1d(self.n_cont)\n",
    "        self.noise = BatchSwapNoise(noise)\n",
    "\n",
    "         \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_input, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "        )\n",
    "          \n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, self.n_input),\n",
    "        )\n",
    "        \n",
    "    \n",
    "    ## function to turn embeddings into label probabilities\n",
    "    def decode_embeddings(self, values, proba=False):\n",
    "\n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        start = 0\n",
    "\n",
    "        out = torch.empty((values.shape[0],0), requires_grad=True).long().to(device)\n",
    "\n",
    "        ### loop through each embedding\n",
    "        ## TODO: there has to be a better way to do this than a loop\n",
    "        for emb in self.embeddings:\n",
    "            \n",
    "            stop = start + emb.weight.data.shape[1]\n",
    "            \n",
    "            pred = torch.empty((0,emb.weight.data.shape[0])).to(device) ## create empty tensor of size (0,n) where n is the number of feature labels\n",
    "                \n",
    "            distance = ((emb.weight.data.unsqueeze(0) - values[:,start:stop].unsqueeze(1))**2).sum(axis=2) \n",
    "\n",
    "            p = softmax(-distance)\n",
    "                    \n",
    "            out = torch.cat([out,p],axis=1)\n",
    "            start = stop  \n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, x_cont, x_cat, encode=False):\n",
    "\n",
    "        x_cat = x_cat.to(device)\n",
    "        x_cont = x_cont.to(device)\n",
    "        \n",
    "        ## add batch swap noise\n",
    "        x_cat = self.noise(x_cat)\n",
    "        x_cont = self.noise(x_cont)\n",
    "\n",
    "        ## embedd categorical features and add dropout\n",
    "        x = [emb(x_cat[:,i]) for i, emb in enumerate(self.embeddings)]   \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        ## batch norm on continuous features\n",
    "        x2 = self.bn_cont(x_cont)\n",
    "        \n",
    "        ## concat cat and cont features\n",
    "        x = torch.cat([x, x2], 1) \n",
    "        \n",
    "        ## encode features\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        ## if I only want to encode features, stop here\n",
    "        if encode:\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        ## decode encoded features\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        ## get continuous features\n",
    "        x_cont_out = x[:,self.n_emb:]\n",
    "        \n",
    "        ## get categorical features and reverse embedding\n",
    "        x_cat_out = x[:,0:self.n_emb]\n",
    "        x_cat_out = self.decode_embeddings(x_cat_out)\n",
    "        \n",
    "\n",
    "        return x_cont_out, x_cat_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b166ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_x = train_dataset.len\n",
    "len_cont = train_dataset.n_cont\n",
    "emb_sz = train_dataset.emb_sz\n",
    "model = AutoEncoder(emb_sz, len_cont).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38f0b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    def forward(self, x_cont_ae, x_cat_ae, x_cont, x_cat):\n",
    "                    \n",
    "        ## loss for the continuous features is simply MSE\n",
    "        cont_loss = self.mse_loss(x_cont_ae, x_cont)\n",
    "        \n",
    "        ## the input categorical features are label encoded, but one-hot encoding is needed to calculate cross entropy loss\n",
    "        one_hot = torch.empty((x_cat.shape[0],0), requires_grad=True).long().to(device)\n",
    "        \n",
    "        for i in range(x_cat.shape[1]):\n",
    "            n_classes = train_dataset.x_cat[:,i].max() + 1 ## not every label is present in a batch, so I have to supply the amount of feature labels   \n",
    "            out = nn.functional.one_hot(x_cat[:,i], num_classes=n_classes).to(device)\n",
    "            one_hot = torch.cat([one_hot,out], axis=1)\n",
    "        \n",
    "        one_hot = one_hot.float()\n",
    "\n",
    "    \n",
    "        ## now I can easily calculate cross entropy loss\n",
    "        cat_loss = self.ce_loss(x_cat_ae, one_hot)\n",
    "        \n",
    "        ## take the sum as the final output\n",
    "        loss = cont_loss + cat_loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed8b2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = customLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = 1e-3,\n",
    "                             weight_decay = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35242603",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c5be1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 180.23. Valid loss: 157.19. Time: 4.75s\n",
      "Epoch 1. Train loss: 136.26. Valid loss: 139.53. Time: 4.68s\n",
      "Epoch 2. Train loss: 128.98. Valid loss: 133.19. Time: 4.69s\n",
      "Epoch 3. Train loss: 125.98. Valid loss: 127.39. Time: 4.69s\n",
      "Epoch 4. Train loss: 124.14. Valid loss: 125.58. Time: 4.69s\n",
      "Epoch 5. Train loss: 122.98. Valid loss: 121.87. Time: 4.68s\n",
      "Epoch 6. Train loss: 122.14. Valid loss: 119.59. Time: 4.70s\n",
      "Epoch 7. Train loss: 121.32. Valid loss: 118.53. Time: 4.68s\n",
      "Epoch 8. Train loss: 120.84. Valid loss: 115.47. Time: 4.66s\n",
      "Epoch 9. Train loss: 120.27. Valid loss: 114.61. Time: 4.66s\n",
      "Epoch 10. Train loss: 119.98. Valid loss: 117.58. Time: 4.67s\n",
      "Epoch 11. Train loss: 119.50. Valid loss: 112.61. Time: 4.67s\n",
      "Epoch 12. Train loss: 119.14. Valid loss: 115.53. Time: 4.67s\n",
      "Epoch 13. Train loss: 118.77. Valid loss: 117.41. Time: 4.68s\n",
      "Epoch 14. Train loss: 118.68. Valid loss: 116.34. Time: 4.69s\n",
      "Epoch 15. Train loss: 118.32. Valid loss: 122.69. Time: 4.68s\n",
      "Epoch 16. Train loss: 118.09. Valid loss: 116.69. Time: 4.68s\n",
      "No improvement for 5 epochs. Ending training.\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    tstart = time.time()\n",
    "    \n",
    "    ## train\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        \n",
    "        x_cont, x_cat = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_cont_ae, x_cat_ae = model(x_cont, x_cat)\n",
    "        \n",
    "        loss = loss_function(x_cont_ae, x_cat_ae, x_cont, x_cat)\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    ## validate\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch_idx, data in enumerate(test_dataloader):\n",
    "        \n",
    "        \n",
    "        x_cont, x_cat = data[0].to(device), data[1].to(device)\n",
    "         \n",
    "        x_cont_ae, x_cat_ae = model(x_cont, x_cat)\n",
    "        \n",
    "        loss = loss_function(x_cont_ae, x_cat_ae, x_cont, x_cat)\n",
    "        \n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_dataloader.dataset))\n",
    "    val_losses.append(valid_loss / len(test_dataloader.dataset))\n",
    " \n",
    "\n",
    "    tend = time.time()\n",
    "    print('Epoch {}. Train loss: {:.2f}. Valid loss: {:.2f}. Time: {:.2f}s'.format(e, train_losses[-1], val_losses[-1], tend-tstart))\n",
    "\n",
    "    \n",
    "    \n",
    "    if (e > patience) & (min(val_losses[-patience:]) != min(val_losses)):\n",
    "        print('No improvement for {} epochs. Ending training.'.format(patience))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a181f",
   "metadata": {},
   "source": [
    "## Preliminary Result\n",
    "The model converges pretty fast and at around 10 epochs it seems to start overfitting. It is a very good sign that training and validation loss are very close. It looks like the model is able to find meaningful representations of the data. <br>\n",
    "I might be able to train for longer with more regularization. <br>\n",
    "I will try to improve performance a bit before moving on to the full dataset, as I expect significantly longer run times with 7x the feature count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f822a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df = pd.DataFrame({'train': train_losses, 'valid': val_losses})\n",
    "ax = sns.lineplot(data=plot_df)\n",
    "ax.set(xlabel='Epoch', ylabel='Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f976b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
